{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guideline parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we must prepare the environment installing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: bs4 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pathvalidate in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (3.2.1)\n",
      "Collecting cleantext\n",
      "  Downloading cleantext-1.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Collecting nltk (from cleantext)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk->cleantext)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->cleantext)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->cleantext)\n",
      "  Downloading regex-2024.7.24-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk->cleantext)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jhyra\\programmazione\\llm\\urology_rag\\.venv\\lib\\site-packages (from click->nltk->cleantext) (0.4.6)\n",
      "Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk, cleantext\n",
      "Successfully installed cleantext-1.1.4 click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.7.24 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 bs4 requests pathvalidate cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to download the first page of the guidelines and parse it to extract the name of all the guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/guidelines/prostate-cancer\n",
      "/guidelines/non-muscle-invasive-bladder-cancer\n",
      "/guidelines/upper-urinary-tract-urothelial-cell-carcinoma\n",
      "/guidelines/muscle-invasive-and-metastatic-bladder-cancer\n",
      "/guidelines/primary-urethral-carcinoma\n",
      "/guidelines/renal-cell-carcinoma\n",
      "/guidelines/testicular-cancer\n",
      "/guidelines/penile-cancer\n",
      "/guidelines/sexual-and-reproductive-health\n",
      "/guidelines/non-neurogenic-female-luts\n",
      "/guidelines/urethral-strictures\n",
      "/guidelines/management-of-non-neurogenic-male-luts\n",
      "/guidelines/chronic-pelvic-pain\n",
      "/guidelines/urological-infections\n",
      "/guidelines/neuro-urology\n",
      "/guidelines/urolithiasis\n",
      "/guidelines/paediatric-urology\n",
      "/guidelines/urological-trauma\n",
      "/guidelines/renal-transplantation\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "website_root = \"https://uroweb.org\" #Guidelines domain\n",
    "folder_guidelines_root = \"/guidelines\"  #Guidelines page\n",
    "guidelines_url = website_root + folder_guidelines_root\n",
    "html_document = requests.get(guidelines_url).text #Webpage downloader, output as json\n",
    "guidelines_index_soup = BeautifulSoup(html_document, 'html.parser') #Creation of a \"soup\" object, make easier to navigate the document\n",
    "all_guidelines_a_class = guidelines_index_soup.findAll(\"a\", {\"class\": \"guideline-card\"}) #Find all link that have the word 'guidelines' inside\n",
    "\n",
    "#Enumerating the guidelines found\n",
    "all_guidelines_url = []\n",
    "for guideline in all_guidelines_a_class:\n",
    "    all_guidelines_url.append(guideline['href'])\n",
    "    print(guideline['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proper indexing and vectorializing, it would be better to not randomly split the text, but to subdivide it.\n",
    "We can do this by splitting the pragraph categorized using an h* token.\n",
    "We can perform this recursivly until each file is under our desired lenght\n",
    "\n",
    "In the following cell, we are going to declare the funcion to perfom the split and declare the variables that regulate the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathvalidate import sanitize_filename\n",
    "\n",
    "#May be worth noticing that if you already have a model running, you can use their tokenizer function to get the real amount of tokens\n",
    "# This works on llama.cpp, different inference software may have different methods\n",
    "## len(llm_model.tokenize(bytes(\"This is a test\", encoding='utf-8')))\n",
    "\n",
    "#We start from <h4> because <h3> are the chapters titles\n",
    "def h4_splitter(\n",
    "        full_paragraph_raw,\n",
    "        chapter_folder_path,\n",
    "        token_subdivider,\n",
    "        maximum_token_lenght,\n",
    "        ):\n",
    "    header_list = ['h3', 'h4']  #Target headers for this split cycle\n",
    "    file_path = os.path.join(\n",
    "        chapter_folder_path, #Chapter folder path from function argument\n",
    "        sanitize_filename(full_paragraph_raw[0].text.replace(\" \", \"_\").lower()[:15]) #Sanitized filename cut to the first 15 characters\n",
    "        )\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "    #We must initialize the variables for the loop\n",
    "    paragraph_loop = 0   #variable to check if we are going to populate the subparagraph of another header\n",
    "    full_sub_paragraph_raw = [] #Subpragraph list\n",
    "    full_paragraph_txt = \"\" #Subparagraph text\n",
    "    #Iterating on each row of the paragraph\n",
    "    for row in full_paragraph_raw:\n",
    "        #If this row and the next one have consecutives target headers, skip this row\n",
    "        if row.name in header_list and row.find_next_sibling().name in header_list:\n",
    "            continue\n",
    "        #If this row and the next one does not have consecutives target headers, proceed\n",
    "        if row.name in header_list and row.find_next_sibling().name not in header_list:\n",
    "            #If there is a loop in progress, we must check if we need to split the paragraph or if we can save the file\n",
    "            if paragraph_loop == 1:\n",
    "                text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "                if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "                    #All these print are for debug purposes, you can remove them\n",
    "                    print(\"Split to h5\")\n",
    "                    print(\"File: \" + filename_sanitized)\n",
    "                    print(\"Lenght: \" + str(text_tokens))\n",
    "                    print (\"\")\n",
    "                    h5_splitter(full_sub_paragraph_raw, file_path, token_subdivider, maximum_token_lenght)  #We call the next split function\n",
    "                    #We reset the variables for a new loop/file\n",
    "                    full_paragraph_txt = \"\"\n",
    "                    full_sub_paragraph_raw = []\n",
    "                    paragraph_loop = 0\n",
    "                else:   #If the token count is lower than the maximum lenght, we save the file\n",
    "                    f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                    f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "                    f.close()   #Close the file\n",
    "                    #We reset the variables for a new loop/file\n",
    "                    full_paragraph_txt = \"\"\n",
    "                    full_sub_paragraph_raw = []\n",
    "                    paragraph_loop = 0\n",
    "            #If there is no loop in progress, we must define the new filename\n",
    "            if paragraph_loop == 0:\n",
    "                full_paragraph_txt += row.text + \". \" #We append a dot and a space at the end of the row\n",
    "                full_sub_paragraph_raw.append(row)  #We add the header and separate it from the rest of the text\n",
    "                filename_sanitized = sanitize_filename(row.text.replace(\" \", \"_\").lower()[:25] + \".txt\") #We sanitize the filename and cut it to the first 25 characters\n",
    "                paragraph_loop = 1   #We set that we are in a loop to populate a subparagraph\n",
    "        #If the row has not a target header, we add the text to the subparagraph\n",
    "        if row.name not in header_list:\n",
    "            full_paragraph_txt += row.text + \" \" #We append a space at the end of the row\n",
    "            full_sub_paragraph_raw.append(row) #We add the paragraph to the full text\n",
    "    #At the end of the for, we must save the last paragraph and reset the variables\n",
    "    text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "    if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "        #All these print are for debug purposes, you can remove them\n",
    "        print(\"Split to h5\")\n",
    "        print(\"File: \" + filename_sanitized)\n",
    "        print(\"Lenght: \" + str(text_tokens))\n",
    "        print (\"\")\n",
    "        h5_splitter(full_sub_paragraph_raw, file_path, token_subdivider, maximum_token_lenght) #We call the next split function\n",
    "    else:   #If the token count is lower than the maximum lenght, we save the file\n",
    "        f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "        f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "        f.close()   #Close the file\n",
    "\n",
    "def h5_splitter(\n",
    "        full_paragraph_raw,\n",
    "        chapter_folder_path,\n",
    "        token_subdivider,\n",
    "        maximum_token_lenght,\n",
    "        ):\n",
    "    header_list = ['h3', 'h4', 'h5']    #Target headers for this split cycle\n",
    "    file_path = os.path.join(chapter_folder_path, sanitize_filename(full_paragraph_raw[0].text.replace(\" \", \"_\").lower()[:15])) #Chapter folder path from function argument\n",
    "    os.makedirs(file_path, exist_ok=True)   #Create the folder if it doesn't exist\n",
    "    #We must initialize the variables for the loop\n",
    "    paragraph_loop = 0\n",
    "    full_sub_paragraph_raw = []\n",
    "    full_paragraph_txt = \"\"\n",
    "    #Iterating on each row of the paragraph\n",
    "    for row in full_paragraph_raw:\n",
    "        #If this row and the next one have consecutives target headers, skip this row\n",
    "        if row.name in header_list and row.find_next_sibling().name in header_list:\n",
    "            continue\n",
    "        #If this row and the next one does not have consecutives target headers, proceed\n",
    "        if row.name in header_list and row.find_next_sibling().name not in header_list:\n",
    "            #If there is a loop in progress, we must check if we need to split the paragraph or if we can save the file\n",
    "            if paragraph_loop == 1:\n",
    "                text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "                if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "                    #All these print are for debug purposes, you can remove them\n",
    "                    print(\"Split to h6\")\n",
    "                    print(\"File: \" + filename_sanitized)\n",
    "                    print(\"Lenght: \" + str(text_tokens))\n",
    "                    print (\"\")\n",
    "                    h6_splitter(full_sub_paragraph_raw, file_path, token_subdivider, maximum_token_lenght)  #We call the next split function\n",
    "                    #We reset the variables for a new loop/file     \n",
    "                    full_paragraph_txt = \"\"\n",
    "                    full_sub_paragraph_raw = []\n",
    "                    paragraph_loop = 0\n",
    "                else:   #If the token count is lower than the maximum lenght, we save the file\n",
    "                    f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                    f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "                    f.close()   #Close the file\n",
    "                    #We reset the variables for a new loop/file\n",
    "                    full_paragraph_txt = \"\"\n",
    "                    full_sub_paragraph_raw = []\n",
    "                    paragraph_loop = 0\n",
    "            #If there is no loop in progress, we must define the new filename\n",
    "            if paragraph_loop == 0:\n",
    "                full_paragraph_txt += row.text + \". \"   #We append a dot and a space at the end of the row\n",
    "                full_sub_paragraph_raw.append(row)  #We add the header and separate it from the rest of the text\n",
    "                filename_sanitized = sanitize_filename(row.text.replace(\" \", \"_\").lower()[:25] + \".txt\") #We sanitize the filename and cut it to the first 25 characters\n",
    "                paragraph_loop = 1   #We set that we are in a loop to populate a subparagraph\n",
    "        #If the row has not a target header, we add the text to the subparagraph\n",
    "        if row.name not in header_list:\n",
    "            full_paragraph_txt += row.text + \" \"    #We append a space at the end of the row\n",
    "            full_sub_paragraph_raw.append(row)      #We add the paragraph to the full text\n",
    "    #At the end of the for, we must save the last paragraph and reset the variables\n",
    "    text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "    if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "        #All these print are for debug purposes, you can remove them\n",
    "        print(\"Split to h6\")\n",
    "        print(\"File: \" + filename_sanitized)\n",
    "        print(\"Lenght: \" + str(text_tokens))\n",
    "        print (\"\")\n",
    "        h6_splitter(full_sub_paragraph_raw, file_path, token_subdivider, maximum_token_lenght)  #We call the next split function\n",
    "    else:   #If the token count is lower than the maximum lenght, we save the file\n",
    "        f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "        f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "        f.close()   #Close the file\n",
    "\n",
    "def h6_splitter(\n",
    "        full_paragraph_raw,\n",
    "        chapter_folder_path,\n",
    "        token_subdivider,\n",
    "        maximum_token_lenght,\n",
    "        ):\n",
    "    header_list = ['h3', 'h4', 'h5', 'h6']  #Target headers for this split cycle\n",
    "    file_path = os.path.join(chapter_folder_path, sanitize_filename(full_paragraph_raw[0].text.replace(\" \", \"_\").lower()[:15])) #Chapter folder path from function argument\n",
    "    os.makedirs(file_path, exist_ok=True)   #Create the folder if it doesn't exist\n",
    "    #We must initialize the variables for the loop\n",
    "    paragraph_loop = 0\n",
    "    full_sub_paragraph_raw = []\n",
    "    full_paragraph_txt = \"\"\n",
    "    #Iterating on each row of the paragraph\n",
    "    for row in full_paragraph_raw:\n",
    "        #If this row and the next one have consecutives target headers, skip this row\n",
    "        if row.name in header_list and row.find_next_sibling().name in header_list:\n",
    "            continue\n",
    "        #If this row and the next one does not have consecutives target headers, proceed\n",
    "        if row.name in header_list and row.find_next_sibling().name not in header_list:\n",
    "            #If there is a loop in progress, we must check if we need to split the paragraph or if we can save the file\n",
    "            if paragraph_loop == 1:\n",
    "                text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "                if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "                    #All these print are for debug purposes, you can remove them\n",
    "                    print(\"Still too long...\" + str(text_tokens))\n",
    "                    print(\"Proceding to half split\")\n",
    "                    print (\"\")\n",
    "                    print(filename_sanitized)\n",
    "                    #Since <h6> is the smallest paragraph I found, to further reduce the lenght we split the paragraph in half\n",
    "                    splitted_string = split_string_in_half(full_paragraph_txt)  #We split the paragraph in half\n",
    "                    i1 = 1  #We initialize the counter for the first split\n",
    "                    #Iterating on the splitted string\n",
    "                    for split in splitted_string:\n",
    "                        text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "                        if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "                            #All these print are for debug purposes, you can remove them\n",
    "                            print(\"Still still too long...\" + str(text_tokens))\n",
    "                            print(\"Proceding to second half split\")\n",
    "                            print (\"\")\n",
    "                            print(filename_sanitized)\n",
    "                            #If the paragraph is still too long, we split it again\n",
    "                            splitted_string2 = split_string_in_half(split)\n",
    "                            i2 = 1  #We initialize the counter for the second split\n",
    "                            #Iterating on the second splitted string\n",
    "                            for split2 in splitted_string2:\n",
    "                                #We do not proceed to further split the file, we save it\n",
    "                                #We could actually proceed to further split the file, but it's not necessary for the purpose of this script\n",
    "                                #In case we need to iterative and endlessy split the file it would be better to implement a recursive function\n",
    "                                #Still, blindly splitting may lead to loss of context and meaning, so a manual check is always recommended and if a ton of splitting is needed, maybe consider using a model with bigger context window or implement a semantic splitting\n",
    "                                f = open(os.path.join(file_path,filename_sanitized.replace(\".txt\", \"\")+\"-\"+str(i1)+\"-\"+str(i2)+\".txt\"), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                                f.write(split2)    #Write the variable on the file\n",
    "                                f.close()   #Close the file\n",
    "                                i2 += 1   #Increment the counter for the second split\n",
    "                        else:\n",
    "                            f = open(os.path.join(file_path,filename_sanitized.replace(\".txt\", \"\")+\"-\"+str(i1)+\".txt\"), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                            f.write(split)  #Write the variable on the file\n",
    "                            f.close()   #Close the file\n",
    "                            i1 += 1  #Increment the counter for the first split\n",
    "                else:\n",
    "                    #Make a subchapter folder, and create it if it doesn't exist\n",
    "                    file_path = os.path.join(chapter_folder_path, sanitize_filename(full_paragraph_raw[0].text.replace(\" \", \"_\").lower()[:15])) #Chapter folder path from function argument\n",
    "                    os.makedirs(file_path, exist_ok=True)   #Create the folder if it doesn't exist\n",
    "                    f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                    f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "                    f.close()   #Close the file\n",
    "                    #We reset the variables for a new loop/file\n",
    "                    full_paragraph_txt = \"\"\n",
    "                    full_sub_paragraph_raw = []\n",
    "                    paragraph_loop = 0\n",
    "            #If there is no loop in progress, we must define the new filename\n",
    "            if paragraph_loop == 0:\n",
    "                full_paragraph_txt += row.text + \". \"   #We append a dot and a space at the end of the row\n",
    "                full_sub_paragraph_raw.append(row)  #We append the row to the full text\n",
    "                filename_sanitized = sanitize_filename(row.text.replace(\" \", \"_\").lower()[:25] + \".txt\")    #We sanitize the filename and cut it to the first 25 characters\n",
    "                paragraph_loop = 1   #We set that we are in a loop to populate a subparagraph\n",
    "        #If the row has not a target header, we add the text to the subparagraph\n",
    "        if row.name not in header_list:\n",
    "            full_paragraph_txt += row.text + \" \"    #We append a space at the end of the row\n",
    "            full_sub_paragraph_raw.append(row)  #We append the row to the full text\n",
    "    #At the end of the for, we must save the last paragraph and reset the variables\n",
    "    text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "    if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "        #All these print are for debug purposes, you can remove them\n",
    "        print(\"Still too long...\" + str(text_tokens))\n",
    "        print(\"Proceding to half split\")\n",
    "        print (\"\")\n",
    "        print(filename_sanitized)\n",
    "        #Since <h6> is the smallest paragraph I found, to further reduce the lenght we split the paragraph in half\n",
    "        splitted_string = split_string_in_half(full_paragraph_txt)\n",
    "        #We must repeat the split cicle because otherwise we are going to lose the last paragraph of each document.\n",
    "        #This could be implemented in a nicer way, separating the splitting loops in their own functions, but for the purpose of this script it's not necessary\n",
    "        i1 = 1  #We initialize the counter for the first split\n",
    "        #We iterate on the splitted string\n",
    "        for split in splitted_string:   \n",
    "            text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "            if text_tokens > maximum_token_lenght: #If the token count is higher than the maximum lenght we set, we must split the paragraph\n",
    "                #All these print are for debug purposes, you can remove them\n",
    "                print(\"Still still too long...\" + str(text_tokens))\n",
    "                print(\"Proceding to second half split\")\n",
    "                print (\"\")\n",
    "                print(filename_sanitized)\n",
    "                #If the paragraph is still too long, we split it again\n",
    "                splitted_string2 = split_string_in_half(split)\n",
    "                i2 = 1  #We initialize the counter for the second split\n",
    "                for split2 in splitted_string2:\n",
    "                    f = open(os.path.join(file_path,filename_sanitized.replace(\".txt\", \"\")+\"-\"+str(i1)+\"-\"+str(i2)+\".txt\"), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                    f.write(split2)    #Write the variable on the file\n",
    "                    f.close()   #Close the file\n",
    "                    i2 += 1 #Increment the counter for the second split\n",
    "            else:\n",
    "                f = open(os.path.join(file_path,filename_sanitized.replace(\".txt\", \"\")+\"-\"+str(i1)+\".txt\"), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                f.write(split)  #Write the variable on the file\n",
    "                f.close()   #Close the file\n",
    "                i1 += 1 #Increment the counter for the first split\n",
    "    else:\n",
    "        f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "        f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "        f.close()   #Close the file\n",
    "\n",
    "#File closure could be delegated to its own function, but for the purpose of this script it's not necessary\n",
    "#A lot of cleanup could be done on this code, but for its limited scope is not necessary. In case webpage structure changes, the code must be updated and maybe this time done properly\n",
    "        \n",
    "#Function to split a string in half\n",
    "def split_string_in_half(s):\n",
    "    string_structure = []  #List to store the two halves of the string\n",
    "    midpoint = len(s) // 2  #Find the midpoint of the string\n",
    "    string_structure.append(s[:midpoint])   #Append the first half of the string\n",
    "    string_structure.append(s[midpoint:])   #Append the second half of the string\n",
    "    return string_structure   #Return the list with the two halves of the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are going to iterate each guidelines url, download, parse the webpage and split it files using the above declared functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-and-aetiology\n",
      "Starting processing: classification-and-staging-systems\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: treatment\n",
      "Split to h5\n",
      "File: 6.2.1._deferred_treatment.txt\n",
      "Lenght: 7511\n",
      "\n",
      "Split to h6\n",
      "File: 6.2.1.2._active_surveilla.txt\n",
      "Lenght: 5249\n",
      "\n",
      "Split to h5\n",
      "File: 6.2.2._radical_prostatect.txt\n",
      "Lenght: 6738\n",
      "\n",
      "Split to h6\n",
      "File: 6.2.2.3._surgical_techniq.txt\n",
      "Lenght: 4266\n",
      "\n",
      "Split to h5\n",
      "File: 6.2.3._radiotherapy.txt\n",
      "Lenght: 8293\n",
      "\n",
      "Split to h6\n",
      "File: 6.2.3.1._external_beam_ra.txt\n",
      "Lenght: 5596\n",
      "\n",
      "Split to h5\n",
      "File: 6.3.5._adjuvant_treatment.txt\n",
      "Lenght: 4111\n",
      "\n",
      "Split to h5\n",
      "File: 6.4.5._treatment_of_psa-o.txt\n",
      "Lenght: 9606\n",
      "\n",
      "Split to h6\n",
      "File: 6.4.5.1._treatment_of_psa.txt\n",
      "Lenght: 6063\n",
      "\n",
      "Split to h5\n",
      "File: 6.7.8._treatment_after_do.txt\n",
      "Lenght: 4594\n",
      "\n",
      "Starting processing: followup\n",
      "Starting processing: quality-of-life-outcomes-in-prostate-cancer\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/prostate-cancer\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: pathological-staging-and-classification-systems\n",
      "Starting processing: diagnosis\n",
      "Starting processing: predicting-disease-recurrence-and-progression\n",
      "Starting processing: disease-management\n",
      "Starting processing: followup-of-patients-with-nmibc\n",
      "Starting processing: patient-reported-outcome-measures-and-quality-indicators-for-nmibc\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/non-muscle-invasive-bladder-cancer\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: staging-and-classification-systems\n",
      "Starting processing: diagnosis\n",
      "Starting processing: prognosis\n",
      "Starting processing: disease-management\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/upper-urinary-tract-urothelial-cell-carcinoma\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: staging-and-classification-systems\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: markers\n",
      "Starting processing: disease-management\n",
      "Split to h5\n",
      "File: 7.7.2.1_definitions_â€˜fit.txt\n",
      "Lenght: 4234\n",
      "\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/muscle-invasive-and-metastatic-bladder-cancer\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: staging-and-classification-systems\n",
      "Starting processing: diagnostic-evaluation-and-staging\n",
      "Starting processing: prognosis\n",
      "Starting processing: disease-management\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/primary-urethral-carcinoma\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: staging-and-classification-systems\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: prognostic-factors\n",
      "Starting processing: disease-management\n",
      "Split to h5\n",
      "File: 7.1.3._radical_and_partia.txt\n",
      "Lenght: 4276\n",
      "\n",
      "Split to h5\n",
      "File: 7.1.4._therapeutic_approa.txt\n",
      "Lenght: 4538\n",
      "\n",
      "Split to h5\n",
      "File: 7.4.3._immunotherapy.txt\n",
      "Lenght: 4377\n",
      "\n",
      "Split to h5\n",
      "File: 7.4.4._therapeutic_strate.txt\n",
      "Lenght: 6173\n",
      "\n",
      "Starting processing: followup-in-rcc\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/renal-cell-carcinoma\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-amp-pathology\n",
      "Starting processing: staging-amp-classification-systems\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: disease-management\n",
      "Starting processing: followup-after-curative-therapy\n",
      "Starting processing: rare-adult-para-and-testicular-tumours\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/testicular-cancer\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathology\n",
      "Starting processing: staging-and-classification-systems\n",
      "Starting processing: diagnostic-evaluation-and-staging\n",
      "Starting processing: disease-management\n",
      "Split to h5\n",
      "File: 6.1.2._treatment_of_invas.txt\n",
      "Lenght: 4326\n",
      "\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/penile-cancer\n",
      "\n",
      "Starting processing: introduction-2\n",
      "Starting processing: methodology\n",
      "Starting processing: male-hypogonadism\n",
      "Starting processing: epidemiology-and-prevalence-of-sexual-dysfunction-and-disorders-of-male-reproductive-health\n",
      "Starting processing: management-of-erectile-dysfunction\n",
      "Starting processing: disorders-of-ejaculation\n",
      "Split to h5\n",
      "File: 6.2.6._disease_management.txt\n",
      "Lenght: 4478\n",
      "\n",
      "Starting processing: low-sexual-desire-and-male-hypoactive-sexual-desire-disorder\n",
      "Starting processing: penile-curvature\n",
      "Split to h5\n",
      "File: 8.2.3._disease_management.txt\n",
      "Lenght: 10587\n",
      "\n",
      "Split to h6\n",
      "File: 8.2.3.1._conservative_tre.txt\n",
      "Lenght: 5308\n",
      "\n",
      "Split to h6\n",
      "File: 8.2.3.2._surgical_treatme.txt\n",
      "Lenght: 5272\n",
      "\n",
      "Starting processing: penile-size-abnormalities-and-dysmorphophobia\n",
      "Split to h5\n",
      "File: 9.3.2._surgical_treatment.txt\n",
      "Lenght: 9002\n",
      "\n",
      "Starting processing: priapism\n",
      "Split to h5\n",
      "File: 10.1.3._disease_managemen.txt\n",
      "Lenght: 6735\n",
      "\n",
      "Starting processing: male-infertility\n",
      "Split to h5\n",
      "File: 11.4.3._varicocele.txt\n",
      "Lenght: 4638\n",
      "\n",
      "Starting processing: late-effects-survivorship-and-mens-health\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/sexual-and-reproductive-health\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: diagnosis\n",
      "Starting processing: disease-management\n",
      "Split to h5\n",
      "File: 4.1.4._disease_management.txt\n",
      "Lenght: 15068\n",
      "\n",
      "Split to h6\n",
      "File: 4.1.4.1._conservative_man.txt\n",
      "Lenght: 4953\n",
      "\n",
      "Split to h6\n",
      "File: 4.1.4.2._pharmacological_.txt\n",
      "Lenght: 6541\n",
      "\n",
      "Split to h5\n",
      "File: 4.2.4._disease_management.txt\n",
      "Lenght: 20325\n",
      "\n",
      "Split to h6\n",
      "File: 4.2.4.1._conservative_man.txt\n",
      "Lenght: 4158\n",
      "\n",
      "Split to h6\n",
      "File: 4.2.4.3._surgical_managem.txt\n",
      "Lenght: 14442\n",
      "\n",
      "Still too long...4582\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...4582\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...4582\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...5328\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...5328\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...5328\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...5427\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...5427\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...5427\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...6607\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...6607\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...6607\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...6974\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...6974\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...6974\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...7856\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...7856\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...7856\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...8139\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8139\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8139\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...8438\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8438\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8438\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...8772\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8772\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8772\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still too long...8943\n",
      "Proceding to half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8943\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Still still too long...8943\n",
      "Proceding to second half split\n",
      "\n",
      "4.2.4.3.2.5._mid-urethral.txt\n",
      "Split to h5\n",
      "File: 4.5.7._surgical_managemen.txt\n",
      "Lenght: 4698\n",
      "\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/non-neurogenic-female-luts\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methodology\n",
      "Starting processing: definition-epidemiology-aetiology-and-prevention\n",
      "Split to h5\n",
      "File: 3.3.1._aetiology_and_prev.txt\n",
      "Lenght: 5075\n",
      "\n",
      "Split to h6\n",
      "File: 3.3.1._aetiology_and_prev.txt\n",
      "Lenght: 5075\n",
      "\n",
      "Still too long...5075\n",
      "Proceding to half split\n",
      "\n",
      "3.3.1._aetiology_and_prev.txt\n",
      "Still still too long...5075\n",
      "Proceding to second half split\n",
      "\n",
      "3.3.1._aetiology_and_prev.txt\n",
      "Still still too long...5075\n",
      "Proceding to second half split\n",
      "\n",
      "3.3.1._aetiology_and_prev.txt\n",
      "Starting processing: classifications\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: disease-management-in-males\n",
      "Split to h5\n",
      "File: 6.2.1._direct_vision_inte.txt\n",
      "Lenght: 4256\n",
      "\n",
      "Split to h5\n",
      "File: 6.3.1._the_role_of_urethr.txt\n",
      "Lenght: 4578\n",
      "\n",
      "Split to h5\n",
      "File: 6.3.2._urethroplasty_for_.txt\n",
      "Lenght: 5318\n",
      "\n",
      "Split to h5\n",
      "File: 6.3.5._posterior_urethra.txt\n",
      "Lenght: 10567\n",
      "\n",
      "Split to h6\n",
      "File: 6.3.5.1._non-traumatic_po.txt\n",
      "Lenght: 6740\n",
      "\n",
      "Starting processing: disease-management-in-females\n",
      "Starting processing: disease-management-in-transgender-patients\n",
      "Starting processing: tissue-transfer\n",
      "Starting processing: perioperative-care-of-urethral-surgery\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/urethral-strictures\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-aetiology-and-pathophysiology\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: disease-management\n",
      "Split to h5\n",
      "File: 5.2.7._combination_therap.txt\n",
      "Lenght: 4608\n",
      "\n",
      "Split to h5\n",
      "File: 5.3.2._enucleation_of_the.txt\n",
      "Lenght: 7031\n",
      "\n",
      "Split to h5\n",
      "File: 5.6.5._surgical_treatment.txt\n",
      "Lenght: 4935\n",
      "\n",
      "Starting processing: followup\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/management-of-non-neurogenic-male-luts\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methodology\n",
      "Starting processing: epidemiology-aetiology-and-pathophysiology\n",
      "Split to h5\n",
      "File: 3.1.5._risk_factors_and_u.txt\n",
      "Lenght: 5176\n",
      "\n",
      "Split to h5\n",
      "File: 3.2.5._risk_factors_and_u.txt\n",
      "Lenght: 6151\n",
      "\n",
      "Starting processing: diagnostic-evaluation\n",
      "Starting processing: management\n",
      "Split to h5\n",
      "File: 5.2.1._drugs_for_chronic_.txt\n",
      "Lenght: 5478\n",
      "\n",
      "Split to h6\n",
      "File: 5.2.1.2._comparisons_of_a.txt\n",
      "Lenght: 5168\n",
      "\n",
      "Still too long...5168\n",
      "Proceding to half split\n",
      "\n",
      "5.2.1.2._comparisons_of_a.txt\n",
      "Still still too long...5168\n",
      "Proceding to second half split\n",
      "\n",
      "5.2.1.2._comparisons_of_a.txt\n",
      "Still still too long...5168\n",
      "Proceding to second half split\n",
      "\n",
      "5.2.1.2._comparisons_of_a.txt\n",
      "Starting processing: evaluation-of-treatment-results\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/chronic-pelvic-pain\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: the-guideline\n",
      "Split to h5\n",
      "File: 3.3.5._disease_management.txt\n",
      "Lenght: 4353\n",
      "\n",
      "Split to h5\n",
      "File: 3.16.2._specific_procedur.txt\n",
      "Lenght: 4539\n",
      "\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/urological-infections\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: the-guideline\n",
      "Split to h5\n",
      "File: 3.4.2._non-invasive_conse.txt\n",
      "Lenght: 5656\n",
      "\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/neuro-urology\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: guidelines\n",
      "Starting processing: metabolic-evaluation-and-recurrence-prevention\n",
      "Starting processing: followup-of-urinary-stones\n",
      "Starting processing: bladder-stones\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/urolithiasis\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: the-guideline\n",
      "Split to h5\n",
      "File: 3.13.4._management.txt\n",
      "Lenght: 6376\n",
      "\n",
      "Split to h5\n",
      "File: 3.16.4._management.txt\n",
      "Lenght: 5132\n",
      "\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/paediatric-urology\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: epidemiology-classification-amp-general-managemnet-principals\n",
      "Starting processing: urogenital-trauma-guidelines\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/urological-trauma\n",
      "\n",
      "Starting processing: introduction\n",
      "Skipping introduction chapter\n",
      "Starting processing: methods\n",
      "Skipping methods chapter\n",
      "Starting processing: the-guideline\n",
      "Split to h5\n",
      "File: 3.1.5._surgical_approache.txt\n",
      "Lenght: 5482\n",
      "\n",
      "Split to h5\n",
      "File: 3.1.7._recipient_complica.txt\n",
      "Lenght: 4759\n",
      "\n",
      "Split to h5\n",
      "File: 3.1.10._immunosuppression.txt\n",
      "Lenght: 5867\n",
      "\n",
      "Starting processing: references\n",
      "Skipping references chapter\n",
      "Starting processing: conflict-of-interest\n",
      "Skipping conflict-of-interest chapter\n",
      "Starting processing: citation-information\n",
      "Skipping citation-information chapter\n",
      "End of guideline: https://uroweb.org/guidelines/renal-transplantation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#We declare the chapters to skip, because they do not contain useful informations and they may pollute data\n",
    "chapters_to_skip = ['introduction', 'methods', 'references', 'conflict-of-interest', 'citation-information']\n",
    "#Since tokens for LLM are about 4 characters long, we can use this to estimate the maximum lenght of the prompt\n",
    "token_subdivider = 4        #We use 4 characters as a token lenght\n",
    "maximum_token_lenght = 4096 #We use 4096 as the maximum token lenght for our model; Must be changed based on your model\n",
    "\n",
    "#We iterate on each guideline\n",
    "for url in all_guidelines_url:\n",
    "    guideline_index_url = website_root + url    #We generate the guideline index url\n",
    "    html_document = requests.get(guideline_index_url).text #Webpage downloader, output as json\n",
    "    guideline_soup = BeautifulSoup(html_document, 'html.parser') #Creation of a \"soup\" object, make easier to navigate the document\n",
    "    guideline_index = guideline_soup.findAll('li', {'class': 'guideline-chapters__item'})   #Find all the chapters of the specific guideline\n",
    "    guideline_name = url.rsplit('/', 1)[-1] #We get the guideline name from the url\n",
    "    guideline_folder = os.path.join(sanitize_filename(folder_guidelines_root), sanitize_filename(guideline_name)) #We generate the folder name\n",
    "    os.makedirs(guideline_folder, exist_ok=True)  #We create the folder\n",
    "    #We initialize the lists with the chapter names and urls\n",
    "    index_name = [] #List with the chapter names\n",
    "    index_url = []  #List with the chapter urls\n",
    "    #We iterate on each chapter\n",
    "    for item in guideline_index:\n",
    "        index_name.append(item.text)        #We append the chapter name to the list\n",
    "        index_url.append(item.a['href'])    #We append the chapter url to the list\n",
    "    number_of_chapters = len(index_name)    #We get the number of chapters\n",
    "    guideline_content_raw = []  #Chapter content list\n",
    "    full_page_raw = []  #Full page content list\n",
    "    #We iterate on each chapter\n",
    "    for i in range(number_of_chapters):\n",
    "        guideline_chapter_url = website_root + index_url[i]   #We generate the chapter url\n",
    "        chapter_title_name_from_url = index_url[i].rsplit('/', 1)[-1] #We get the chapter name from the url\n",
    "        print(\"Starting processing: \" + chapter_title_name_from_url) #Print for debug purposes\n",
    "        #We skip the chapters we listed aboce\n",
    "        if chapter_title_name_from_url not in chapters_to_skip:\n",
    "            html_document = requests.get(guideline_chapter_url).text #Webpage downloader, output as json\n",
    "            guideline_chapter_soup = BeautifulSoup(html_document, 'html.parser') #Creation of a \"soup\" object, make easier to navigate the document\n",
    "            #The next few steps could be done in a single line, but for clarity purpose I split them\n",
    "            guideline_text = guideline_chapter_soup.find('article', {'class': 'guideline-text'}) #Find the article tag with the guideline text\n",
    "            article_text_in_generator = guideline_text.contents #Get the content of the article\n",
    "            chapter_title = article_text_in_generator[0].text   #Get the chapter title\n",
    "            chapter_folder = chapter_title.lower().replace(\" \", \"_\") #Generate the chapter folder name\n",
    "            chapter_folder_path = os.path.join(guideline_folder, sanitize_filename(chapter_folder)) #Generate the chapter folder path\n",
    "            os.makedirs(chapter_folder_path, exist_ok=True)  #Create the folder\n",
    "            filename_sanitized = \"_PLACEHOLDER_FILENAME_\"   #Initialize the filename variable\n",
    "            definitive_path = \"_PLACEHOLDER_PATHNAME_\"   #Initialize the definitive path variable\n",
    "            full_paragraph_raw = []    #Initialize the full paragraph list\n",
    "            full_paragraph_txt = \"_PLACEHOLDER_CONTENT_\"    #Initialize the full paragraph text variable\n",
    "            full_sub_paragraph_txt = \"\" #Initialize the full subparagraph text variable\n",
    "            in_paragraph_loop = 0   #Initialize the variable to check if we are populating a subparagraph\n",
    "            #Loop on the chapter content\n",
    "            for row in article_text_in_generator:   #Iterating on each row of the chapter\n",
    "                if row.name == 'h2':    #If it's an header <h2> we can skip it\n",
    "                    continue\n",
    "                if row.name == 'h3':    #If it's an header <h3> we must check if we are populating a subparagraph <h4>\n",
    "                    if in_paragraph_loop == 1: #If we are populating a subparagraph <h4> and we reach an header <h3> we must close the subparagraph and save it\n",
    "                        text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "                        #Se il testo Ã¨ troppo lungo, dividiamo con <h4>\n",
    "                        if text_tokens > maximum_token_lenght:\n",
    "                            h4_splitter(full_paragraph_raw, chapter_folder_path, token_subdivider, maximum_token_lenght)    #We call the first split function\n",
    "                            #We reset the variables for a new loop/file\n",
    "                            full_paragraph_raw = []\n",
    "                            in_paragraph_loop = 0\n",
    "                        else:\n",
    "                            #if the file is short enough, we save it as a single file\n",
    "                            file_path = chapter_folder_path   #We set the file path\n",
    "                            os.makedirs(file_path, exist_ok=True)   #Create the folder if it doesn't exist\n",
    "                            f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Creiamo/apriamo il file\n",
    "                            f.write(full_paragraph_txt)    #Scriviamo la variabile su file\n",
    "                            f.close()   #Chiudiamo il file\n",
    "                            in_paragraph_loop = 0   #Ripristiniamo la variabile per un nuovo loop/file\n",
    "                    #The if is placed as second, so we don't have to repeat the code in the previous condition\n",
    "                    #We must check if we are populating a subparagraph <h4> or if we are starting a new paragraph\n",
    "                    if in_paragraph_loop == 0:\n",
    "                        filename_sanitized = sanitize_filename(row.text.replace(\" \", \"_\").lower()[:25] + \".txt\")  #Sanitize the filename and cut it to the first 25 characters\n",
    "                        full_paragraph_txt = row.text + \". \"  #We append a dot and a space at the end of the row\n",
    "                        full_paragraph_raw = []    #Initialize the full paragraph list\n",
    "                        full_paragraph_raw.append(row)  #We append the header to the full text\n",
    "                        in_paragraph_loop = 1   #We set that we are in a loop to populate a subparagraph    \n",
    "                else:   #If it's not an header <h3> we must add the text to the paragraph\n",
    "                    full_paragraph_txt += row.text + \" \"  #We append a space at the end of the row\n",
    "                    full_paragraph_raw.append(row)  #We append the row to the full text\n",
    "            #At the end of the loop we must check if we are populating a subparagraph <h4> and save it\n",
    "            #We must check the lenght of the praragraph to split it if it's too long\n",
    "            text_tokens = len(full_paragraph_txt)//token_subdivider #Count the tokens of the chapter subdividing the text by the aproximated token lenght\n",
    "            #Se il testo Ã¨ troppo lungo, dividiamo con <h4>\n",
    "            if text_tokens > maximum_token_lenght:\n",
    "                h4_splitter(full_paragraph_raw, chapter_folder_path, token_subdivider, maximum_token_lenght)    #We call the first split function\n",
    "                #We reset the variables for a new loop/file\n",
    "                full_paragraph_raw = []\n",
    "                in_paragraph_loop = 0 \n",
    "            else:\n",
    "                #if the file is short enough, we save it as a single file\n",
    "                file_path = chapter_folder_path\n",
    "                os.makedirs(file_path, exist_ok=True)   #Create the folder if it doesn't exist\n",
    "                f = open(os.path.join(file_path,filename_sanitized), \"w\", encoding=\"utf-8\")  #Create/open the file\n",
    "                f.write(full_paragraph_txt)    #Write the variable on the file\n",
    "                f.close()   #Close the file\n",
    "                in_paragraph_loop = 0   #Reset the variable for a new loop/file\n",
    "        #We skip the chapters we listed aboce\n",
    "        if chapter_title_name_from_url in chapters_to_skip:\n",
    "            print(\"Skipping \" + index_url[i].rsplit('/', 1)[-1] + \" chapter\")\n",
    "\n",
    "    print(\"End of guideline: \" + guideline_index_url + \"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once extracted, the text must be cleaned.\n",
    "We do this in two separate step to allow editing in a non destructive way, allowing to modify the cleaning parameters without the need of redownloading everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleantext\n",
    "import re\n",
    "import glob\n",
    "\n",
    "#Define the path to the files\n",
    "txt_folder_path = sanitize_filename(folder_guidelines_root) #Folder path of the guidelines, inherited from the previous functions\n",
    "processed_file_path = \"rag_sources\" #Folder path for the processed files\n",
    "\n",
    "#Define the function to clean the text\n",
    "def txt_processer (file_path):    \n",
    "    extracted_text = open(file_path, \"r\", encoding=\"utf-8\").read()  #Open the file and read the content\n",
    "    extracted_text_without_citations = re.sub(r'\\[\\d+(,\\d+)*\\]', '', extracted_text) #Remove the citations numbers\n",
    "    cleaned_text = cleantext.clean(\n",
    "            extracted_text_without_citations,\n",
    "            clean_all=False,    #If True, it will set all the others parameters to True\n",
    "            extra_spaces=True,  #Remove extra white spaces\n",
    "            stemming=False,     #Perform stemming\n",
    "            stopwords=True,     #Removal of selected language stopwords\n",
    "            stp_lang='english', #Stopwords language\n",
    "            lowercase=True,     #Lowercase all the text\n",
    "        )\n",
    "    return cleaned_text\n",
    "\n",
    "#Create the folder if it doesn't exist\n",
    "os.makedirs(processed_file_path, exist_ok=True)\n",
    "#Iterating on each file in the main directory\n",
    "for filename in glob.iglob(txt_folder_path + \"/**/**.txt\", recursive=True):\n",
    "    #os.path.split splits the path in two parts, the folders [0] and the file [1]\n",
    "    document_file_name = os.path.split(filename)[1] #Get the file name, including the extension\n",
    "    #We define the folder path of the file replacing the txt_folder_path with processed_file_path\n",
    "    processed_folder_path = os.path.split(filename)[0].replace(txt_folder_path, processed_file_path)\n",
    "    #We create the folder if it doesn't exist\n",
    "    os.makedirs(processed_folder_path, exist_ok=True)\n",
    "    cleaned_text = txt_processer(filename) #Invoke the function to clean the text\n",
    "    file = open(filename.replace(txt_folder_path, processed_file_path), \"w\", encoding=\"utf-8\")  #Create/open the file, we replace the folder path with the processed one\n",
    "    file.writelines(cleaned_text)   #Write the cleaned text on the file\n",
    "    file.close()    #Close the file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
